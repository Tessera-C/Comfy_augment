{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comfy_script.runtime import *\n",
    "load()\n",
    "from comfy_script.runtime.nodes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import jh_path\n",
    "import os\n",
    "import jh_prompts\n",
    "import jh_TiROD\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class_mapping = {\n",
    "    # \"road\": \"road\",\n",
    "    # \"sidewalk\": 1,\n",
    "    \"parking\": \"parked cars\",\n",
    "    # \"rail track\": 3,\n",
    "    \"person\": \"pedestrian\",\n",
    "    # \"rider\": 5,\n",
    "    # \"car\": \"car\",\n",
    "    \"truck\": \"truck\",\n",
    "    \"bus\": \"bus\",\n",
    "    # \"on rails\": 9,\n",
    "    \"motorcycle\": \"motorcycle\",\n",
    "    \"bicycle\": \"bicycle\",\n",
    "    \"caravan\": \"caravan\",\n",
    "    \"trailer\": \"trailer\",\n",
    "    \"building\": \"building\",\n",
    "    # \"wall\": \"wall\",\n",
    "    \"fence\": \"fence\",\n",
    "    \"guard rail\": \"guard rail\",\n",
    "    \"bridge\": \"bridge\",\n",
    "    \"tunnel\": \"tunnel\",\n",
    "    \"pole\": \"pole\",\n",
    "    # \"pole group\": 21,\n",
    "    \"traffic sign\": \"traffic sign\",\n",
    "    \"traffic light\": \"traffic light\",\n",
    "    \"plants\": \"plants\",\n",
    "    # \"vegetation\": 24,\n",
    "    # \"terrain\": 25,\n",
    "    # \"sky\": 26,\n",
    "    # \"ground\": 27,\n",
    "    # \"dynamic\": 28,\n",
    "    # \"static\": 29,\n",
    "}\n",
    "\n",
    "# Dictionary input to prompt\n",
    "def generate_sdxl_prompt(object_counts):\n",
    "    if not object_counts:\n",
    "        return \"\"\n",
    "\n",
    "    # 이름 변경 매핑\n",
    "    name_map = {\n",
    "        \"bottle\": \"plastic water bottle\"\n",
    "    }\n",
    "\n",
    "    prompt_parts = []\n",
    "    for name, count in object_counts.items():\n",
    "        # 매핑된 이름 사용\n",
    "        display_name = name_map.get(name, name)\n",
    "\n",
    "        if count == 1:\n",
    "            description = f\"a single {display_name}\"\n",
    "        else:\n",
    "            # 간단한 복수형 처리\n",
    "            plural_name = display_name if display_name.endswith('s') else display_name + 's'\n",
    "            description = f\"{count} {plural_name}\"\n",
    "        prompt_parts.append(description)\n",
    "\n",
    "    return ', '.join(prompt_parts)\n",
    "\n",
    "\n",
    "# YOLO class names\n",
    "yolo_names = [\n",
    "    'slippers', 'stool', 'wire', 'carpet', 'sofa', 'socks', 'feces',\n",
    "    'table', 'bed', 'closetool', 'book', 'cabinet', 'trashcan', 'curtain'\n",
    "]\n",
    "\n",
    "# 클래스 이름 매핑 (선택적 변경)\n",
    "name_map = {\n",
    "    'wire': 'electrical wire',\n",
    "    'feces': 'feces on floor',\n",
    "    'closetool': 'toilet',\n",
    "    'stool': 'stool, chair'\n",
    "}\n",
    "\n",
    "def load_yolo_annotation(yolo_txt_path):\n",
    "    \"\"\"\n",
    "    YOLO .txt 주석 파일에서 class_id만 추출해서 카운팅함\n",
    "    \"\"\"\n",
    "    object_counts = Counter()\n",
    "\n",
    "    if not os.path.isfile(yolo_txt_path):\n",
    "        return {}\n",
    "\n",
    "    with open(yolo_txt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            try:\n",
    "                class_id = int(parts[0])\n",
    "                class_name = yolo_names[class_id]\n",
    "                mapped_name = name_map.get(class_name, class_name)\n",
    "                object_counts[mapped_name] += 1\n",
    "            except (IndexError, ValueError):\n",
    "                continue  # 잘못된 줄은 건너뜀\n",
    "\n",
    "    return dict(object_counts)\n",
    "\n",
    "def generate_prompt_YOLO(object_counts):\n",
    "    if not object_counts:\n",
    "        return \"\"\n",
    "\n",
    "    prompt_parts = []\n",
    "    for name, count in object_counts.items():\n",
    "        if count == 1:\n",
    "            description = f\"a {name}\"\n",
    "        else:\n",
    "            plural_name = name if name.endswith('s') else name + 's'\n",
    "            description = f\"{count} {plural_name}\"\n",
    "        prompt_parts.append(description)\n",
    "\n",
    "    return ', '.join(prompt_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes Dataset Generation Code (img2img, with double controlnet)\n",
    "\n",
    "img_list = jh_path.get_all_image_paths('input/Image_in')\n",
    "print(len(img_list))\n",
    "\n",
    "\n",
    "for img in img_list[0:1]:\n",
    "    replaced_img = img.replace(\"Image_in\", \"Image_anno\")\n",
    "    replaced_img = replaced_img.replace(\"leftImg8bit\", \"gtFine_polygons\")\n",
    "    base, ext = os.path.splitext(replaced_img)\n",
    "    json_path = base + '.json'\n",
    "\n",
    "    img_path = img.replace(\"input/\", \"\")\n",
    "    # prompt from annotation\n",
    "    obj_prompt = jh_path.label_from_json(json_path, class_mapping, 2000)\n",
    "    # prompt concatenate\n",
    "    pos_prompt_basic = '''high-quality photo, candid shot, high quality photorealism, road surface, urban roads, car, detailed, life-like texture, driver's view, naturally vibrant color, city landscape, detailed building and nature background, clear focus, diversity, car bonnet and hood ornament partially visible at bottom'''\n",
    "    pos_prompt = pos_prompt_basic + obj_prompt\n",
    "    neg_prompt = '(worst quality, low quality, illustration, drawing, 2d, painting, cartoons, sketch), low resolution, noisy, distortion, light flare, low-quality texture, over-saturation, jpeg artifact, over-polished, blur, bokeh, monotone'\n",
    "    print(obj_prompt)\n",
    "    print(img_path)\n",
    "    img_filename = os.path.basename(img_path)\n",
    "\n",
    "    with Workflow():\n",
    "        model, clip, vae = CheckpointLoaderSimple('realvisxlV50_v50Bakedvae.safetensors')\n",
    "        model = VectorscopeCC(model, True, 0, 0, 1, 0, 0, 0, 'Ones', '1 - Cos')\n",
    "        model, clip = LoraLoader(model, clip, 'sd_xl_offset_example-lora_1.0.safetensors', 0.7, 0.8)\n",
    "        positive_prompt_conditioning = CLIPTextEncode(pos_prompt, clip)\n",
    "        negative_prompt_conditioning = CLIPTextEncode(neg_prompt, clip)\n",
    "        control_net = ControlNetLoader('control-lora-canny-rank256.safetensors')\n",
    "        image, _ = LoadImageFromPath(img_path)\n",
    "        image2 = CannyEdgePreprocessor(image, 40, 100, 1024)\n",
    "        apply_control_net_canny_positive, apply_control_net_canny_negative = ControlNetApplyAdvanced(positive_prompt_conditioning, negative_prompt_conditioning, control_net, image2, 0.6, 0, 0.8, None)\n",
    "        control_net2 = ControlNetLoader('control-lora-depth-rank256.safetensors')\n",
    "        image3 = DepthAnythingV2Preprocessor(image, 'depth_anything_v2_vits.pth', 1024)\n",
    "        apply_control_net_depth_positive, apply_control_net_depth_negative = ControlNetApplyAdvanced(apply_control_net_canny_positive, apply_control_net_canny_negative, control_net2, image3, 0.8, 0, 0.9, vae)\n",
    "        latent = VAEEncode(image, vae)\n",
    "        latent = KSampler(model, random.randint(1,999999999999), 40, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.92)\n",
    "        image4 = VAEDecode(latent, vae)\n",
    "        save_filename = os.path.splitext(img_filename)[0]\n",
    "        SaveImage(\n",
    "                image4,\n",
    "                \"cityscapes4/\" + save_filename,\n",
    "                use_counter = True  )\n",
    "\n",
    "    time.sleep(45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes Dataset Generation Code (img2img, with triple controlnet) - segmentation contronet added\n",
    "\n",
    "img_list = jh_path.get_all_image_paths('input/Image_in')\n",
    "print(\"Total number of Images: \" + str(len(img_list)))\n",
    "\n",
    "part_list = img_list[2000:]\n",
    "cycle_len = len(part_list)\n",
    "\n",
    "for i, img in enumerate(part_list):\n",
    "    current_index = i + 1\n",
    "    replaced_img = img.replace(\"Image_in\", \"Image_anno\")\n",
    "    replaced_img = replaced_img.replace(\"leftImg8bit\", \"gtFine_polygons\")\n",
    "    base, ext = os.path.splitext(replaced_img)\n",
    "    json_path = base + '.json'\n",
    "\n",
    "    seg_img = img.replace(\"Image_in\", \"Image_anno\")\n",
    "    seg_img = seg_img.replace(\"leftImg8bit\", \"gtFine_color\")\n",
    "    seg_img = seg_img.replace(\"input/\", \"\")\n",
    "\n",
    "    img_path = img.replace(\"input/\", \"\")\n",
    "    # prompt from annotation\n",
    "    obj_prompt = jh_path.label_from_json(json_path, class_mapping, 2000)\n",
    "    # prompt concatenate\n",
    "    pos_prompt_basic = '''high-quality photo, road, candid shot, high quality photorealism, road surface, urban roads, car, detailed, life-like texture, driver's view, naturally vibrant color, city landscape, detailed building and nature background, clear focus, pedestrian, asphalt road, buildings, traffic signs, street tree, diversity, car bonnet and hood ornament partially visible at bottom'''\n",
    "    pos_prompt = pos_prompt_basic + obj_prompt\n",
    "    neg_prompt = '(worst quality, low quality, illustration, drawing, 2d, painting, cartoons, sketch), low resolution, noisy, distortion, light flare, low-quality texture, over-saturation, jpeg artifact, over-polished, blur, bokeh'\n",
    "    print(obj_prompt)\n",
    "    print(f\"{current_index} is running among {cycle_len}\")\n",
    "    print(img_path)\n",
    "    img_filename = os.path.basename(img_path)\n",
    "\n",
    "    with Workflow():\n",
    "        model, clip, vae = CheckpointLoaderSimple('realvisxlV50_v50Bakedvae.safetensors')\n",
    "        model = VectorscopeCC(model, True, 0, 0, 1, 0, 0, 0, 'Ones', '1 - Cos')\n",
    "        model, clip = LoraLoader(model, clip, 'sd_xl_offset_example-lora_1.0.safetensors', 0.7000000000000001, 0.8)\n",
    "        positive_prompt_conditioning = CLIPTextEncode(pos_prompt, clip)\n",
    "        negative_prompt_conditioning = CLIPTextEncode(neg_prompt, clip)\n",
    "        control_net = ControlNetLoader('diffusion_pytorch_model_promax.safetensors')\n",
    "        image, _ = LoadImage(seg_img)\n",
    "        positive, negative = ControlNetApplyAdvanced(positive_prompt_conditioning, negative_prompt_conditioning, control_net, image, 0.4, 0, 0.8, None)\n",
    "        control_net2 = ControlNetLoader('control-lora-canny-rank256.safetensors')\n",
    "        image2, _ = LoadImage(img_path)\n",
    "        image3 = CannyEdgePreprocessor(image2, 40, 100, 1024)\n",
    "        apply_control_net_canny_positive, apply_control_net_canny_negative = ControlNetApplyAdvanced(positive, negative, control_net2, image3, 0.6, 0, 0.8, None)\n",
    "        control_net3 = ControlNetLoader('control-lora-depth-rank256.safetensors')\n",
    "        image4 = DepthAnythingV2Preprocessor(image2, 'depth_anything_v2_vits.pth', 1024)\n",
    "        apply_control_net_depth_positive, apply_control_net_depth_negative = ControlNetApplyAdvanced(apply_control_net_canny_positive, apply_control_net_canny_negative, control_net3, image4, 0.7000000000000001, 0, 0.8, vae)\n",
    "        latent = VAEEncode(image2, vae)\n",
    "        latent = KSampler(model, random.randint(1,999999999999999), 40, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.92)\n",
    "        image5 = VAEDecode(latent, vae)\n",
    "        save_filename = os.path.splitext(img_filename)[0]\n",
    "        SaveImage(\n",
    "            image5,\n",
    "            \"cityscapes4/\" + save_filename,\n",
    "            use_counter = False  )\n",
    "\n",
    "    time.sleep(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TiROD & CID-SIMS Dataset Generation Code (img2img, with double controlnet) - LLAVA Captioner implemented\n",
    "\n",
    "img_list = jh_path.get_all_image_paths('input/TiROD/Domain1/High/images/train')\n",
    "print(\"Total number of Images: \" + str(len(img_list)))\n",
    "\n",
    "# Partial Running\n",
    "part_list = img_list[0:]\n",
    "cycle_len = len(part_list)\n",
    "\n",
    "# JSON load\n",
    "replaced_img = img_list[0].replace(\"images\", \"annotations\")\n",
    "dir_path, file_name = os.path.split(replaced_img)\n",
    "parent_path, train_folder = os.path.split(dir_path)\n",
    "json_path = os.path.join(parent_path, \"train.json\")\n",
    "\n",
    "for i, img in enumerate(part_list):\n",
    "    current_index = i + 1\n",
    "    print(img)\n",
    "    # JSON prompt part\n",
    "    filename_to_image_id, category_id_to_name, image_annotations = jh_TiROD.load_annotation_data(json_path)\n",
    "    obj_dict = jh_TiROD.get_object_counts_for_image(os.path.basename(img), filename_to_image_id, category_id_to_name, image_annotations)\n",
    "    obj_prompt = generate_sdxl_prompt(obj_dict)\n",
    "    print(obj_prompt)\n",
    "    \n",
    "    img_path = img.replace(\"input/\", \"\")\n",
    "    \n",
    "    # prompt concatenate\n",
    "    pos_prompt_basic = ', high-quality photo, candid shot, high quality photorealism, detailed, life-like texture, low-angle indoor shot, only described objects'\n",
    "    neg_prompt = '(worst quality, low quality, illustration, drawing, painting, cartoons, sketch), low resolution, noisy, distortion, UnrealisticDream, over-saturation, over-polished, blur, duplicated objects'\n",
    "    \n",
    "    print(f\"{current_index} is running among {cycle_len}\")\n",
    "    # print(img_path)\n",
    "    img_filename = os.path.basename(img_path)\n",
    "\n",
    "    llava_prompt = '''Describe the locations of objects in the image using short phrases. Use expressions like \"bottle on the right\", \"potted plant in the corner\", \"traffic cone near the box\". Only output simple object-location phrases separated by commas.'''\n",
    "\n",
    "    with Workflow():\n",
    "        model, clip, vae = CheckpointLoaderSimple('realisticVisionV60B1_v51VAE.safetensors')\n",
    "        model = VectorscopeCC(model, True, 0, 0, 1, 0, 0, 0, 'Ones', '1 - Cos')\n",
    "        positive_prompt_basic_prompt = SeargePromptText(pos_prompt_basic)\n",
    "        image, _ = LoadImage(img_path)\n",
    "\n",
    "        # string = LlavaCaptioner(image, 'llava-v1.5-7b-Q4_K', 'llava-v1.5-7b-mmproj-Q4_0', llava_prompt, 100, 0.2)\n",
    "        # string = ShowTextPysssss(string)\n",
    "\n",
    "        # # # ACTIVATE to use LLAVA Captioner\n",
    "        # string2 = TextConcatenate(r'\\n', 'true', positive_prompt_basic_prompt, string, '', '')\n",
    "\n",
    "        string2 = TextConcatenate(r'\\n', 'true', obj_prompt, positive_prompt_basic_prompt, '', '')\n",
    "        positive_prompt_conditioning = CLIPTextEncode(string2, clip)\n",
    "        negative_prompt_conditioning = CLIPTextEncode(neg_prompt, clip)\n",
    "        control_net = ControlNetLoader('control_v11p_sd15_canny.pth')\n",
    "        upscale_model = UpscaleModelLoader('RealESRGAN_x2.pth')\n",
    "        image2 = ImageUpscaleWithModel(upscale_model, image)\n",
    "        image3 = CannyEdgePreprocessor(image2, 20, 80, 640)\n",
    "        apply_control_net_canny_positive, apply_control_net_canny_negative = ControlNetApplyAdvanced(positive_prompt_conditioning, negative_prompt_conditioning, control_net, image3, 0.7, 0, 0.8, vae)\n",
    "        control_net2 = ControlNetLoader('control_v11f1p_sd15_depth.pth')\n",
    "        image4 = DepthAnythingV2Preprocessor(image2, 'depth_anything_v2_vitb.pth', 1024)\n",
    "        apply_control_net_depth_positive, apply_control_net_depth_negative = ControlNetApplyAdvanced(apply_control_net_canny_positive, apply_control_net_canny_negative, control_net2, image4, 0.8, 0, 0.7, vae)\n",
    "        latent = VAEEncode(image, vae)\n",
    "        # RANDOM SEED\n",
    "        # latent = KSampler(model, random.randint(1,999999999999999), 40, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.54)\n",
    "\n",
    "        # UNIFORM SEED\n",
    "        latent = KSampler(model, 555999869199, 40, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.55)\n",
    "        image5 = VAEDecode(latent, vae)\n",
    "        save_filename = os.path.splitext(img_filename)[0]\n",
    "        SaveImage(\n",
    "            image5,\n",
    "            \"TiROD_jsonprompt_uniform_5/\" + save_filename,\n",
    "            use_counter = False  )\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        time.sleep(20)\n",
    "    else:\n",
    "        time.sleep(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TiROD & CID-SIMS Dataset Generation Code (img2img, with TRIPLE controlnet) - LLAVA Captioner implemented\n",
    "\n",
    "img_list = jh_path.get_all_image_paths('input/TiROD/Domain1/High/images/train')\n",
    "print(\"Total number of Images: \" + str(len(img_list)))\n",
    "\n",
    "# Partial Running\n",
    "part_list = img_list[0:]\n",
    "cycle_len = len(part_list)\n",
    "\n",
    "# JSON load\n",
    "replaced_img = img_list[0].replace(\"images\", \"annotations\")\n",
    "dir_path, file_name = os.path.split(replaced_img)\n",
    "parent_path, train_folder = os.path.split(dir_path)\n",
    "json_path = os.path.join(parent_path, \"train.json\")\n",
    "\n",
    "for i, img in enumerate(part_list):\n",
    "    current_index = i + 1\n",
    "    print(img)\n",
    "    # JSON prompt part\n",
    "    filename_to_image_id, category_id_to_name, image_annotations = jh_TiROD.load_annotation_data(json_path)\n",
    "    obj_dict = jh_TiROD.get_object_counts_for_image(os.path.basename(img), filename_to_image_id, category_id_to_name, image_annotations)\n",
    "    obj_prompt = generate_sdxl_prompt(obj_dict)\n",
    "    print(obj_prompt)\n",
    "    \n",
    "    img_path = img.replace(\"input/\", \"\")\n",
    "    \n",
    "    # prompt concatenate\n",
    "    pos_prompt_basic = ', high-quality photo, candid shot, high quality photorealism, detailed, life-like texture, low-angle indoor shot, only described objects'\n",
    "    neg_prompt = '(worst quality, low quality, illustration, drawing, painting, cartoons, sketch), low resolution, noisy, distortion, UnrealisticDream, over-saturation, over-polished, blur, duplicated objects'\n",
    "    \n",
    "    print(f\"{current_index} is running among {cycle_len}\")\n",
    "    # print(img_path)\n",
    "    img_filename = os.path.basename(img_path)\n",
    "\n",
    "    llava_prompt = '''Describe the locations of objects in the image using short phrases. Use expressions like \"bottle on the right\", \"potted plant in the corner\", \"traffic cone near the box\". Only output simple object-location phrases separated by commas.'''\n",
    "\n",
    "    with Workflow():\n",
    "        model, clip, vae = CheckpointLoaderSimple('realisticVisionV60B1_v51VAE.safetensors')\n",
    "        model = VectorscopeCC(model, True, 0, 0, 1, 0, 0, 0, 'Ones', '1 - Cos')\n",
    "        positive_prompt_basic_prompt = SeargePromptText(pos_prompt_basic)\n",
    "        image, _ = LoadImage(img_path)\n",
    "\n",
    "        # string = LlavaCaptioner(image, 'llava-v1.5-7b-Q4_K', 'llava-v1.5-7b-mmproj-Q4_0', llava_prompt, 100, 0.2)\n",
    "        # string = ShowTextPysssss(string)\n",
    "\n",
    "        # # # ACTIVATE to use LLAVA Captioner\n",
    "        # string2 = TextConcatenate(r'\\n', 'true', positive_prompt_basic_prompt, string, '', '')\n",
    "\n",
    "        string2 = TextConcatenate(r'\\n', 'true', obj_prompt, positive_prompt_basic_prompt, '', '')\n",
    "        positive_prompt_conditioning = CLIPTextEncode(string2, clip)\n",
    "        negative_prompt_conditioning = CLIPTextEncode(neg_prompt, clip)\n",
    "        control_net = ControlNetLoader('controlnet_sd15_HED.safetensors')\n",
    "        upscale_model = UpscaleModelLoader('RealESRGAN_x2.pth')\n",
    "        image2 = ImageUpscaleWithModel(upscale_model, image)\n",
    "        image3 = HEDPreprocessor(image2, 'enable', 640)\n",
    "        \n",
    "        apply_control_net_hed_positive, apply_control_net_hed_negative = ControlNetApplyAdvanced(positive_prompt_conditioning, negative_prompt_conditioning, control_net, image3, 0.7, 0, 0.8, vae)\n",
    "        control_net2 = ControlNetLoader('control_v11p_sd15_canny.pth')\n",
    "        image4 = CannyEdgePreprocessor(image2, 20, 80, 960)\n",
    "        apply_control_net_canny_positive, apply_control_net_canny_negative = ControlNetApplyAdvanced(apply_control_net_hed_positive, apply_control_net_hed_negative, control_net2, image4, 0.8, 0, 0.7, vae)\n",
    "\n",
    "        control_net3 = ControlNetLoader('control_v11f1p_sd15_depth.pth')\n",
    "        image5 = DepthAnythingV2Preprocessor(image2, 'depth_anything_v2_vitb.pth', 1024)\n",
    "        apply_control_net_depth_positive, apply_control_net_depth_negative = ControlNetApplyAdvanced(apply_control_net_canny_positive, apply_control_net_canny_negative, control_net3, image5, 0.8, 0, 0.7, vae)\n",
    "        latent = VAEEncode(image, vae)\n",
    "        # RANDOM SEED\n",
    "        # latent = KSampler(model, random.randint(1,999999999999999), 40, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.54)\n",
    "\n",
    "        # UNIFORM SEED\n",
    "        latent = KSampler(model, 7894792311789, 40, 4, 'dpmpp_sde', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.50)\n",
    "        image5 = VAEDecode(latent, vae)\n",
    "        save_filename = os.path.splitext(img_filename)[0]\n",
    "        SaveImage(\n",
    "            image5,\n",
    "            \"TiROD_jsonprompt_uniform_9/\" + save_filename,\n",
    "            use_counter = False  )\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        time.sleep(20)\n",
    "    else:\n",
    "        time.sleep(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODSR-IHS Dataset Generation Code \n",
    "\n",
    "img_list = jh_path.get_all_image_paths('input/ODSR')\n",
    "print(\"Total number of Images: \" + str(len(img_list)))\n",
    "\n",
    "# Partial Running\n",
    "part_list = img_list[0:]\n",
    "cycle_len = len(part_list)\n",
    "\n",
    "\n",
    "for i, img in enumerate(part_list):\n",
    "    current_index = i + 1\n",
    "    print(img)\n",
    "    # YOLO prompt part\n",
    "    txt_path = os.path.splitext(img)[0] + \".txt\"\n",
    "    obj_dictionary = load_yolo_annotation(txt_path)\n",
    "    obj_prompt = generate_prompt_YOLO(obj_dictionary)\n",
    "    print(obj_prompt)\n",
    "    \n",
    "    img_path = img.replace(\"input/\", \"\")\n",
    "    \n",
    "    # prompt concatenate\n",
    "    pos_prompt_basic = 'high-quality photo, candid shot, high quality photorealism, detailed, life-like texture, low-angle indoor shot, '\n",
    "    neg_prompt = '(worst quality, low quality, illustration, drawing, painting, cartoons, sketch), low resolution, noisy, distortion, UnrealisticDream, over-saturation, over-polished, blur, text, uniform color, monotone'\n",
    "    pos_prompt_final = pos_prompt_basic + obj_prompt\n",
    "    print(pos_prompt_final)\n",
    "    print(f\"{current_index} is running among {cycle_len}\")\n",
    "    # print(img_path)\n",
    "    img_filename = os.path.basename(img_path)\n",
    "\n",
    "    with Workflow():\n",
    "        model, clip, vae = CheckpointLoaderSimple('realvisxlV50_v50Bakedvae.safetensors')\n",
    "        model = VectorscopeCC(model, True, 0, 0, 1, 0, 0, 0, 'Ones', '1 - Cos')\n",
    "        positive_prompt_basic_prompt = SeargePromptText(pos_prompt_basic)\n",
    "        image, _ = LoadImage(img_path)\n",
    "\n",
    "        # string = LlavaCaptioner(image, 'llava-v1.5-7b-Q4_K', 'llava-v1.5-7b-mmproj-Q4_0', llava_prompt, 100, 0.2)\n",
    "        # string = ShowTextPysssss(string)\n",
    "\n",
    "        # # # ACTIVATE to use LLAVA Captioner\n",
    "        # string2 = TextConcatenate(r'\\n', 'true', positive_prompt_basic_prompt, string, '', '')\n",
    "\n",
    "        string2 = TextConcatenate(r'\\n', 'true', obj_prompt, positive_prompt_basic_prompt, '', '')\n",
    "        positive_prompt_conditioning = CLIPTextEncode(string2, clip)\n",
    "        negative_prompt_conditioning = CLIPTextEncode(neg_prompt, clip)\n",
    "\n",
    "        control_net = ControlNetLoader('control-lora-canny-rank256.safetensors')\n",
    "        image2 = CannyEdgePreprocessor(image, 90, 160, 512)\n",
    "        apply_control_net_canny_positive, apply_control_net_canny_negative = ControlNetApplyAdvanced(positive_prompt_conditioning, negative_prompt_conditioning, control_net, image2, 0.8, 0, 0.8, vae)\n",
    "        control_net2 = ControlNetLoader('control-lora-depth-rank256.safetensors')\n",
    "        upscale_model = UpscaleModelLoader('RealESRGAN_x2.pth')\n",
    "        image3 = ImageUpscaleWithModel(upscale_model, image)\n",
    "        image4 = DepthAnythingV2Preprocessor(image3, 'depth_anything_v2_vitb.pth', 1024)\n",
    "        apply_control_net_depth_positive, apply_control_net_depth_negative = ControlNetApplyAdvanced(apply_control_net_canny_positive, apply_control_net_canny_negative, control_net2, image4, 0.9, 0, 0.8, vae)\n",
    "        latent = VAEEncode(image3, vae)\n",
    "        # RANDOM SEED\n",
    "        # latent = KSampler(model, random.randint(1,999999999999999), 40, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.9)\n",
    "\n",
    "        # UNIFORM SEED\n",
    "        latent = KSampler(model, random.randint(1,999999999999999), 50, 5, 'dpmpp_2m', 'karras', apply_control_net_depth_positive, apply_control_net_depth_negative, latent, 0.80)\n",
    "        image5 = VAEDecode(latent, vae)\n",
    "        image6 = ImageResize(image5, 'rescale', 'true', 'lanczos', 0.5, 416, 416)\n",
    "        save_filename = os.path.splitext(img_filename)[0]\n",
    "        SaveImage(\n",
    "            image6,\n",
    "            \"ODSR_v7/\" + save_filename,\n",
    "            use_counter = False  )\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        time.sleep(25)\n",
    "    else:\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
